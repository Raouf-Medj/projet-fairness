{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendu Final Projet Fairness en IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étudiant 01 : MEDJADJ Mohamed Abderraouf <br>\n",
    "Étudiant 02 : KERMADJ Zineddine <br>\n",
    "Groupe : 01 <br>\n",
    "Parcours : LDD3 Magistère d'Informatique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Objectif du projet :\n",
    "L’objectif de ce projet est d’**analyser un sous-ensemble de métadonnées et d’images** du NIH Chest X-ray Dataset, comprenant environ 5300 points de données, afin d’**identifier d’éventuels biais**. Après avoir appliqué une méthode de prétraitement pour **réduire ces biais et améliorer l’équité des données**, nous entraînerons un **modèle de classification d’images**. Enfin, nous analyserons **l’impact de la pondération sur les performances du modèle** ainsi que **l’effet du post-traitement** sur l’atténuation des biais.\n",
    "### b. Description du dataset :\n",
    "*Le NIH Chest X-ray Dataset est un vaste ensemble de données médicales comprenant **112 120 images** de radiographies thoraciques issues de **30 805 patients uniques**, avec des étiquettes de maladies générées par traitement automatique du langage naturel (**NLP**) à partir des rapports radiologiques. Ce jeu de données vise à pallier le **manque d’images médicales annotées**, un obstacle majeur au développement de systèmes de diagnostic assisté par ordinateur (CAD) cliniquement pertinents. Les étiquettes sont estimées à plus de **90 % de précision**, rendant cet ensemble adapté à l’apprentissage faiblement supervisé. Avant sa publication, le plus grand jeu de données disponible comptait seulement 4 143 images. Plus de détails sur l’ensemble de données et le processus d’annotation sont disponibles dans l’article en libre accès : « ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases » (Wang et al.).*\n",
    "### c. Contenu du dataset :\n",
    "***Image Index** : Identifiant unique pour chaque image. <br>\n",
    "**Finding Labels** : Diagnostiques associés à l'image (plusieurs diagnostics peuvent être présents). <br>\n",
    "**Follow-up** # : Le numéro de suivi, indiquant si l'image appartient à un suivi ou à une première consultation. <br>\n",
    "**Patient ID** : Identifiant unique pour chaque patient. <br>\n",
    "**Patient Age** : L'âge du patient. <br>\n",
    "**Patient Gender** : Le genre du patient. <br>\n",
    "**View Position** : La position de l'image (par exemple, AP pour antéro-postérieur). <br>\n",
    "**Dimensions et espacements de l'image** : Ces informations peuvent être utiles pour l'analyse des images, mais elles ne semblent pas directement liées à l'identification des biais.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOMMAIRE:\n",
    "<pre><b>\n",
    "I.   Introduction\n",
    "II.    0. Fonctions Utilitaires\n",
    "II.    1. Préparation des données\n",
    "II.    2. Analyse des données\n",
    "II.    3. Identification des biais\n",
    "III. Application des méthodes de preprocessing\n",
    "IV.  Application des méthodes de postprocessing\n",
    "V.   Analyse et compréhension\n",
    "VI.  Conclusion\n",
    "</b></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **REMARQUES IMPORTANTES:**\n",
    "### *a. Les explications des sorties, commentaires des graphiques, etc, sont inclus dans des cellules de **code, et pas markdown**, veuillez donc s'il vous plaît ne pas ignorer les lignes commentées.*\n",
    "\n",
    "### *b. Pour l'entrainement, veuillez consulter \"training.ipynb\".*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.0. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Import des librairies nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.sklearn.metrics import *\n",
    "from aif360.algorithms.preprocessing import *\n",
    "from train_classifieur import train_classifier, pred_classifier\n",
    "from scipy.sparse import csr_matrix\n",
    "import random\n",
    "from IPython.display import Image, display\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage, display\n",
    "from aif360.algorithms.postprocessing.calibrated_eq_odds_postprocessing import CalibratedEqOddsPostprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Les fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour le calcul des métriques de fairness\n",
    "\n",
    "def get_group_metrics(\n",
    "    y_true,\n",
    "    y_pred=None,\n",
    "    prot_attr=None,\n",
    "    priv_group=1,\n",
    "    pos_label=1,\n",
    "    sample_weight=None,\n",
    "):\n",
    "    group_metrics = {}\n",
    "    group_metrics[\"base_rate\"] = base_rate(\n",
    "        y_true=y_true, pos_label=pos_label, sample_weight=sample_weight\n",
    "    )\n",
    "    group_metrics[\"statistical_parity_difference\"] = statistical_parity_difference(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "    )\n",
    "    group_metrics[\"disparate_impact_ratio\"] = disparate_impact_ratio(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "    )\n",
    "    if not y_pred is None:\n",
    "        group_metrics[\"equal_opportunity_difference\"] = equal_opportunity_difference(\n",
    "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"average_odds_difference\"] = average_odds_difference(\n",
    "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, priv_group=priv_group, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"conditional_demographic_disparity\"] = conditional_demographic_disparity(\n",
    "            y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"smoothed_edf\"] = smoothed_edf(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "        group_metrics[\"df_bias_amplification\"] = df_bias_amplification(\n",
    "        y_true=y_true, y_pred=y_pred, prot_attr=prot_attr, pos_label=pos_label, sample_weight=sample_weight\n",
    "        )\n",
    "    return group_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour tracer des matrices de confusion\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels=[\"sain\", \"malade\"], normalize=False, title=\"Matrice de confusion\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / len(y_true) * 100\n",
    "    \n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    \n",
    "    fig = px.imshow(cm_df, \n",
    "                    labels=dict(x=\"Prédiction\", y=\"Vérité\", color=\"Fréquence (%)\" if normalize else \"Fréquence\"), \n",
    "                    x=labels, \n",
    "                    y=labels, \n",
    "                    color_continuous_scale='Blues',\n",
    "                    range_color=[0, 100] if normalize else None) \n",
    "    \n",
    "    for i in range(len(cm_df)):\n",
    "        for j in range(len(cm_df.columns)):\n",
    "            fig.add_annotation(\n",
    "                x=j,\n",
    "                y=i,\n",
    "                text=f'{cm_df.iloc[i, j]:.2f}%' if normalize else f'{cm_df.iloc[i, j]}',\n",
    "                showarrow=False,\n",
    "                font=dict(color=\"black\", size=14),\n",
    "                align=\"center\"\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(title=title, xaxis_title=\"Prédiction\", yaxis_title=\"Vérité\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour afficher des matrices de confusion séparées pour chaque groupe défini par group_columns\n",
    "def plot_confusion_matrix_by_group(y_true, y_pred, df, group_columns, labels=None, normalize=False):\n",
    "    for group_value, group_df in df.groupby(group_columns):\n",
    "        y_true_group = y_true[group_df.index]\n",
    "        y_pred_group = y_pred[group_df.index]\n",
    "        \n",
    "        print(f\"Matrice de confusion pour {group_columns}: {group_value}\")\n",
    "        plot_confusion_matrix(y_true_group, y_pred_group, labels, normalize, title=f\"Matrice de Confusion ({group_columns}={group_value})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1. Preparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./DATA\"\n",
    "PREDS_DIR = \"./expe_log\"\n",
    "df = pd.read_csv(PREDS_DIR+\"/preds.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Exploration préliminaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "# Affichons les 5 premiers points de donnée du dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifions les types de données et les valeurs manquantes\n",
    "df.info()\n",
    "\n",
    "# Explication des sorties:\n",
    "# Le dataset est composé de 12 colonnes, dont 8 correspondent à des features numériques, et 4 catégorielles.\n",
    "# Toutes les colonnes ne contiennent pas de valeurs nulles sauf la dernière (Unnamed: 11), qui contient que des valeurs nulles.\n",
    "# Le dataset contient 54009 points de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "df.describe()\n",
    "\n",
    "# Explication des sorties:\n",
    "# Max: On remarque qu'il y a une valeur max = 412 pour l'age, qui n'est pas normal (outlier), et qui peut être dû à une erreur de frappe.\n",
    "# Count = nombre de points de données sauf pour 'Unnamed: 11', cette colonne contient que des null.\n",
    "# Mean: Moyenne des valeurs par colonne, pas de remarque importante.\n",
    "# Std: Standard deviation des valeurs par colonne, pas de remarque importante.\n",
    "# Min: Le min des valeurs par colonne, pas de remarque importante.\n",
    "# Les Quantiles: pas de remarque importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Préparation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sauvegarde le dataframe original avant toute transformation\n",
    "original_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour faire des train-test split:\n",
    "\n",
    "def train_test_split(df):\n",
    "    train_sain_path = DATA_DIR+\"/train/sain\"\n",
    "    train_malade_path = DATA_DIR+\"/train/malade\"\n",
    "\n",
    "    train_images = set(os.listdir(train_sain_path) + os.listdir(train_malade_path))\n",
    "\n",
    "    df[\"in_train\"] = df[\"Image Index\"].apply(lambda x: 1 if x in train_images else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Élimination des outliers (age > 130 years old)\n",
    "df = df[df['Patient Age'] <= 130]\n",
    "\n",
    "# Séparation des données train-test\n",
    "train_test_split(df)\n",
    "\n",
    "# Séparation des colonnes liées aux images des autres métadonnées\n",
    "df_image_related = df[['Image Index', 'OriginalImage[Width', 'Height]', 'OriginalImagePixelSpacing[x', 'y]', 'View Position']]\n",
    "df_others = df.drop(columns=['Image Index', \n",
    "    'OriginalImage[Width', 'Height]', 'OriginalImagePixelSpacing[x', 'y]', 'View Position'])\n",
    "\n",
    "# Encodage de 'preds' et 'labels' en binaire\n",
    "df_others[\"preds\"] = df_others[\"preds\"].map({\"sain\": 0, \"malade\": 1}) \n",
    "df_others[\"labels\"] = df_others[\"labels\"].map({\"sain\": 0, \"malade\": 1})\n",
    "\n",
    "# Déplacement du label 'Finding Labels' à la fin du dataframe.\n",
    "df_others = df_others[[col for col in df_others.columns if col != 'Finding Labels'] + ['Finding Labels']]\n",
    "df_original_label = df_others[['Finding Labels']]\n",
    "\n",
    "print(df_others.shape)\n",
    "# On remarque que le nombre de points de données a diminué de 6 (outliers eliminés).\n",
    "\n",
    "df_others.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 1: encodage one-hot du label\n",
    "\n",
    "# a. Création d'une colonne 'Finding Labels' contenant une liste des labels\n",
    "df_encoded_OH = df_others.copy()\n",
    "df_encoded_OH['Finding Labels'] = df_encoded_OH['Finding Labels'].replace('No Finding', '').str.split('|')\n",
    "df_encoded_OH['Finding Labels'] = df_encoded_OH['Finding Labels'].apply(lambda x: [] if x == [''] else x)\n",
    "\n",
    "# b. Encodage des colonnes en One-Hot avec le prefix \"Finding_\"\n",
    "all_labels = set([label for sublist in df_encoded_OH['Finding Labels'] for label in sublist])\n",
    "for label in all_labels:\n",
    "    column_name = f\"Finding_{label.lower().replace(' ', '_')}\"\n",
    "    df_encoded_OH[column_name] = df_encoded_OH['Finding Labels'].apply(lambda x: 1 if label in x else 0)\n",
    "df_encoded_OH = df_encoded_OH.drop(columns=['Finding Labels'])\n",
    "\n",
    "df_encoded_OH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 2: encodage binaire (1 ssi patient malade) du label\n",
    "\n",
    "df_encoded_bool = df_others.copy()\n",
    "df_encoded_bool = df_encoded_bool.drop(columns=['Finding Labels'])\n",
    "\n",
    "df_encoded_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2. Analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Séparation du label, des features numériques et catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_encoded_OH\n",
    "df_encoded.dtypes\n",
    "# Toutes les features sont numériques, sauf le genre (sexe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation du label, des features numériques et catégorielles\n",
    "one_hot_label = [col for col in df_encoded.columns if col.startswith(\"Finding\")] # pour la version 1 de l'encodage\n",
    "boolean_label = 'labels' # pour la version 2 de l'encodage\n",
    "\n",
    "numerical_features = list(set(df_encoded.select_dtypes(include=np.number).columns) - set(one_hot_label))\n",
    "categorical_features = list(set(df_encoded.columns) - set(numerical_features) - set(one_hot_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse univariée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Analyse de la corrélation (Feature-Label et Feature-Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui calcule la corrélation linéaire entre deux features\n",
    "def compute_correlation(df, cola, colb):\n",
    "  return np.corrcoef(df[cola].values, df[colb].values)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des corrélations entre les features et les colonnes du label (one-hot) (Feature-Label).\n",
    "for num_feature in numerical_features:\n",
    "  if num_feature not in {\"labels\", \"preds\", \"in_train\"}:\n",
    "    for label_col in one_hot_label:\n",
    "      corr = compute_correlation(df_encoded, label_col, num_feature)\n",
    "      if np.abs(corr) > 0.05 :\n",
    "        print(num_feature, label_col, corr)\n",
    "\n",
    "## On n'affiche que les couples de features qui ont une corrélation (valeur absolue) supérieure à 0.05\n",
    "## Comme on le voit, il s'agit de très faibles corrélations (pas de biais linéaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des corrélations entre les features et les colonnes du label (booléen) (Feature-Label).\n",
    "for num_feature in numerical_features:\n",
    "    if num_feature not in {\"labels\", \"preds\", \"in_train\"}:\n",
    "        corr = compute_correlation(df_encoded_bool, boolean_label, num_feature)\n",
    "        if np.abs(corr) > 0.05 :\n",
    "            print(num_feature, boolean_label, corr)\n",
    "\n",
    "## On n'affiche que les couples de features qui ont une corrélation (valeur absolue) supérieure à 0.05\n",
    "## Comme on le voit, il s'agit de très faibles corrélations (pas de biais linéaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des corrélations entre les features (Feature-Feature).\n",
    "for num_feature in numerical_features:\n",
    "  if num_feature not in {\"labels\", \"preds\", \"in_train\"}:\n",
    "    for num_feature2 in (set(numerical_features)-set([num_feature])):\n",
    "      if num_feature2 not in {\"labels\", \"preds\", \"in_train\"}:\n",
    "        corr = compute_correlation(df_encoded, num_feature, num_feature2)\n",
    "        if np.abs(corr) > 0.05 :\n",
    "          print(num_feature, num_feature2, corr)\n",
    "\n",
    "## On n'affiche que les couples de features qui ont une corrélation (valeur absolue) supérieure à 0.05\n",
    "## Comme on le voit, il s'agit de très faibles corrélations (pas de biais linéaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Analyse des biais liés au sexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df_encoded, names=\"Patient Gender\", title=\"Distribution du sexe\")\n",
    "fig.show()\n",
    "\n",
    "# Explication des sorties:\n",
    "# On remarque que le nombre de points de données 'masculin' est plus élevé que celui des 'féminin', \n",
    "# ce qui pourrait indiquer une sous-représentation des femmes dans le dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df_encoded_bool, x='Patient Gender', color=boolean_label, \\\n",
    "                  title=f\"{'Patient Gender'} distribution by {boolean_label}\")\n",
    "fig.show()\n",
    "\n",
    "# Explication des sorties:\n",
    "# On remarque que la distribution du sexe selon 'is_ill' est balancée (50% malade, 50% non malade, dans les deux sexes).\n",
    "# Donc on est bon vis-à-vis de ça."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_col_bool = (df_encoded_bool['Patient Gender'] == 'M') # Conversion {'F' -> 0, 'M' -> 1}\n",
    "\n",
    "def calc_dir_gender(df, label):\n",
    "    dir = disparate_impact_ratio(\n",
    "        y_true=df[label],\n",
    "        prot_attr=gender_col_bool,\n",
    "        pos_label=0\n",
    "    )\n",
    "    return dir\n",
    "\n",
    "print(\"global_is_ill\", calc_dir_gender(df_encoded_bool, boolean_label))\n",
    "\n",
    "# Explication des sorties:\n",
    "# DIR ~ 1, donc pas de biais pas de classe (M/F) favorisée vis-à-vis le label 'is_ill'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_categorical_hist(df, cat_feature, target):\n",
    "  fig = px.histogram(df, x=cat_feature, color=target)\n",
    "  fig.show()\n",
    "\n",
    "def display_categorical_hist_percent(df, cat_feature, target):\n",
    "  if cat_feature != \"labels\" and cat_feature != \"preds\" :\n",
    "    df_summarized = df.groupby([target,cat_feature]).agg(\"count\").reset_index()\n",
    "    df_summarized[f\"percent of {cat_feature}\"] = df_summarized[[cat_feature,\"Patient Age\"]].apply(\n",
    "      lambda x: 100*x[\"Patient Age\"]/df_summarized[df_summarized[cat_feature]==x[cat_feature]][\"Patient Age\"].sum(), axis=1\n",
    "    )\n",
    "    df_summarized[target] = df_summarized[target].astype(str)\n",
    "    fig = px.bar(df_summarized, x=f\"{cat_feature}\", y=f\"percent of {cat_feature}\", color=target)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_feature in categorical_features:\n",
    "  for label_col in one_hot_label:\n",
    "    display_categorical_hist_percent(df_encoded, cat_feature, label_col)\n",
    "\n",
    "# Explication des sorties:\n",
    "# En général, il n'y a pas de classe favorisée, la distribution de chaque pathologie est similaire sur les deux sexes.\n",
    "# Mais on remarque qu'il y a des maladies sous-représentées, confirmons cela avec la visualisation suivante (partie 3.c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_col in one_hot_label:\n",
    "    dir = calc_dir_gender(df_encoded, label_col)\n",
    "    print(label_col, dir)\n",
    "\n",
    "# Explication des sorties:\n",
    "# Tous les DIRs calculés sont ~ 1, donc pas de biais pas de classe (M/F) favorisée vis-à-vis les labels one-hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Analyse des biais liés à la distribution des labels (one-hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = df_encoded[one_hot_label].sum()\n",
    "fig = px.bar(\n",
    "    x=category_counts.index,\n",
    "    y=category_counts.values,\n",
    "    labels={'x': 'Disease Type', 'y': 'Count of 1s'},\n",
    "    title='Distribution of One-Hot Encoded Labels'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Explication des sorties:\n",
    "# On remarque une distribution non équilibrée sur les différentes maladies, ce qui peut induire un modèle biaisé.\n",
    "# Solution: introduire des weights, détaillons ça après."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Analyse des biais liés à l'age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df_encoded_bool, x=boolean_label, y='Patient Age', title=f\"age distribution by {boolean_label}\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(df_encoded_bool, x='Patient Age', color=boolean_label, barmode=\"overlay\", \\\n",
    "                  title=f\"age distribution by {boolean_label}\")\n",
    "fig.show()\n",
    "\n",
    "# Explication des sorties:\n",
    "# On remarque que 50% des patients qui sont condensés au mileu ~[30ans -> 65ans].\n",
    "# Les distributions d'age sur les deux classes (malade/non malade) sont similaires.\n",
    "# Mais on pourrait argumenter que les catégories d'age ~[0 - 30 ans] et ~[65+ ans], sont sous-représentés.\n",
    "# Pour le confirmer, passons au graphique suivant (Discretisation d'age)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretisation de l'age\n",
    "discrete_age = {}\n",
    "discrete_age['age_group'] = pd.cut(df_encoded_bool['Patient Age'], bins=[0, 30, 65, 130], labels=['0-30', '30-65', '65+'], right=True)\n",
    "df_encoded_bool['age_group'] = discrete_age['age_group']\n",
    "fig = px.histogram(discrete_age, x='age_group', title='Distribution des classes d\\'age',\n",
    "                   labels={'age_group': 'Classes d\\'age'}, \n",
    "                   text_auto=True)\n",
    "fig.show()\n",
    "\n",
    "# Explication des sorties:\n",
    "# Voilà, on voit bien le deséquilibre dans la distribution des classes d'age, les classes '0-30' et '65+' sont sous-représentées\n",
    "# en comparaison avec '30-65', d'où la présence d'un potentiel biais lié à ça."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinons le unpriviliged group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On encode les classes d'age en booléen comme suit:\n",
    "\n",
    "df_encoded_bool[\"age_30_65\"] = (df_encoded_bool['age_group'] == '30-65').astype(int)\n",
    "df_encoded_bool = df_encoded_bool.drop(columns=['age_group'])\n",
    "df_encoded_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Analyse des biais liés au \"Patient ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df_encoded_bool, x=boolean_label, y='Patient ID', title=f\"patient_id distribution by {boolean_label}\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.histogram(df_encoded_bool, x='Patient ID', color=boolean_label, \\\n",
    "                  title=f\"patient_id distribution by {boolean_label}\")\n",
    "fig.show()\n",
    "\n",
    "# Explication des sorties:\n",
    "# On peut dire de la première figure que la distribution des patient_id est similaire sur les deux classes (malade/non malade).\n",
    "# On peut même dire que la distribution est pseudo-uniforme.\n",
    "# Mais en regardant le deuxième graphique, on remarque une petite anomalie au voisinage de l'id 25000, \n",
    "# et qui peut biaiser le modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse bivariée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Analyse (bivariée) des biais liés au 'Follow-up'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fig 01\n",
    "fig = px.histogram(df_encoded, x=\"Patient ID\", y=\"Follow-up #\", title=\"Distribution du nombre de suivis\")\n",
    "fig.show()\n",
    "\n",
    "# Fig 02\n",
    "fig = px.histogram(df_encoded, x=\"Patient Gender\", y=\"Follow-up #\", title=\"Nombre de suivis par genre\")\n",
    "fig.show()\n",
    "\n",
    "# Fig 03\n",
    "fig = px.histogram(df_encoded, x=\"Patient Age\", y=\"Follow-up #\", title=\"Relation entre l’âge et le nombre de suivis\")\n",
    "fig.show()\n",
    "\n",
    "# Fig 03\n",
    "df_melted = df_encoded.melt(id_vars=[\"Follow-up #\"], value_vars=[col for col in df_encoded.columns if \"Finding_\" in col], \n",
    "                     var_name=\"disease\", value_name=\"presence\")\n",
    "df_melted = df_melted[df_melted[\"presence\"] == 1]\n",
    "\n",
    "# Fig 04\n",
    "fig = px.histogram(df_melted, x=\"disease\", y=\"Follow-up #\", title=\"Nombre de suivis par maladie\")\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Explication des sorties:\n",
    "# Fig 01: Deux petites anomalies au voisinage des IDs 10000 (pic très marqué du nombre de suivis) et 25000 (forte baisse, presque à zéro), \n",
    "# cela peut biaiser le modèle quand on prédit des valeurs à ces voisinages.\n",
    "# Fig 02: Même remarque que dans l'étude des biais liés au sexe (b).\n",
    "# Fig 03: Même remarque que dans l'étude des biais liés à l'age (d).\n",
    "# Fig 04: Même remarque que dans l'étude des biais liés à la distribution des labels one-hot (c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finding_columns = [col for col in df_encoded.columns if col.startswith(\"Finding\")]\n",
    "\n",
    "sparse_matrix = csr_matrix(df_encoded[finding_columns].values)\n",
    "\n",
    "co_occurrence_matrix = sparse_matrix.T @ sparse_matrix\n",
    "\n",
    "total_occurrences = np.array(sparse_matrix.sum(axis=0)).flatten()\n",
    "probability_matrix = co_occurrence_matrix.toarray() / total_occurrences\n",
    "\n",
    "co_occurrence_df = pd.DataFrame(co_occurrence_matrix.toarray(), index=finding_columns, columns=finding_columns)\n",
    "probability_df = pd.DataFrame(probability_matrix, index=finding_columns, columns=finding_columns)\n",
    "\n",
    "def display_matrix(df, title):\n",
    "    return df.style.background_gradient(cmap=\"Blues\", axis=None).set_caption(title)\n",
    "\n",
    "styled_co_occurrence_df = display_matrix(co_occurrence_df, \"Matrice de Co-Occurrence\")\n",
    "styled_probability_df = display_matrix(probability_df, \"Matrice de Probabilités Conditionnelles\")\n",
    "\n",
    "print(\"Matrice de co-occurrence:\")\n",
    "display(styled_co_occurrence_df)\n",
    "\n",
    "print(\"\\nMatrice de probabilités conditionnelles:\")\n",
    "display(styled_probability_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse d'images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La principale nouveauté depuis le mi-projet est l’intégration d’images dans le dataset. Cependant, ces nouvelles données peuvent introduire de nouveaux biais, tels qu’une faible luminosité, la présence de dispositifs médicaux intra-corporels, ou d'autres artefacts visuels susceptibles d’influencer l’analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Liste de chemins vers tes dossiers (paths)\n",
    "paths = [\n",
    "    \"DATA/train/sain\",\n",
    "    \"DATA/train/malade\",\n",
    "    \"DATA/valid/sain\",\n",
    "    \"DATA/valid/malade\",\n",
    "]\n",
    "\n",
    "# Récupère tous les fichiers .png dans les dossiers\n",
    "toutes_les_images = []\n",
    "for path in paths:\n",
    "    if os.path.exists(path):\n",
    "        images = [os.path.join(path, f) for f in os.listdir(path) if f.lower().endswith('.png')]\n",
    "        toutes_les_images.extend(images)\n",
    "\n",
    "# Choisir un certain nombre d'images aléatoires (par exemple 6)\n",
    "nombre_images = 6\n",
    "images_choisies = random.sample(toutes_les_images, min(nombre_images, len(toutes_les_images)))\n",
    "\n",
    "# Affiche les images dans le notebook\n",
    "for chemin in images_choisies:\n",
    "    display(Image(filename=chemin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "# Fonction pour calculer la luminosité moyenne d'une image\n",
    "def calculer_luminosite(image_path):\n",
    "    # Ouvrir l'image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Convertir l'image en niveaux de gris (pour analyser la luminosité)\n",
    "    grayscale_img = img.convert(\"L\")\n",
    "    \n",
    "    # Convertir l'image en un tableau numpy\n",
    "    img_array = np.array(grayscale_img)\n",
    "    \n",
    "    # Calculer la luminosité moyenne de l'image\n",
    "    luminosite_moyenne = np.mean(img_array)\n",
    "    \n",
    "    return luminosite_moyenne\n",
    "\n",
    "# Fonction pour récupérer toutes les images dans un répertoire\n",
    "def recuperer_images_du_repertoire(dossier):\n",
    "    images = []\n",
    "    for file in os.listdir(dossier):\n",
    "        if file.lower().endswith('.png'):  # Seulement les fichiers .png\n",
    "            images.append(os.path.join(dossier, file))\n",
    "    return images\n",
    "\n",
    "# Variable pour stocker les images à faible et forte luminosité\n",
    "images_faible_luminosite = []\n",
    "images_forte_luminosite = []\n",
    "\n",
    "# Pour chaque dossier, récupérer toutes les images et choisir celle ayant la plus faible luminosité\n",
    "for path in paths:\n",
    "    if os.path.exists(path):\n",
    "        # Récupérer toutes les images du dossier\n",
    "        images = recuperer_images_du_repertoire(path)\n",
    "        \n",
    "        if images:\n",
    "            # Calculer la luminosité pour chaque image\n",
    "            luminosites = [(image, calculer_luminosite(image)) for image in images]\n",
    "            \n",
    "            # Trouver l'image avec la luminosité la plus faible\n",
    "            image_min_luminosite = min(luminosites, key=lambda x: x[1])\n",
    "            image_max_luminosite = max(luminosites, key=lambda x: x[1])\n",
    "            \n",
    "            # Ajouter l'image à faible et forte luminosité à la liste\n",
    "            images_faible_luminosite.append(image_min_luminosite[0])\n",
    "            images_forte_luminosite.append(image_max_luminosite[0])\n",
    "\n",
    "# Affichage des images à faible luminosité\n",
    "for chemin in images_faible_luminosite:\n",
    "    display(IPImage(filename=chemin))\n",
    "    \n",
    "for chemin in images_forte_luminosite:\n",
    "    display(IPImage(filename=chemin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreground_filter(gris, threshold):\n",
    "    \"\"\"\n",
    "    Retourne un masque booléen indiquant quels pixels de l'image en niveaux de gris (gris)\n",
    "    ont une valeur inférieure au seuil.\n",
    "    \"\"\"\n",
    "    return gris < threshold\n",
    "\n",
    "def lightness(img: Image.Image) -> float:\n",
    "    \"\"\"\n",
    "    Calcule la \"faible luminosité\" d'une image PIL.\n",
    "    L'image est convertie en niveaux de gris par la moyenne des canaux R, G et B.\n",
    "    La fonction retourne la moyenne des pixels du premier plan (où la luminosité est inférieure au seuil).\n",
    "    \"\"\"\n",
    "    M = np.array(img)\n",
    "    \n",
    "    gris = np.mean(M, axis=2)\n",
    "    \n",
    "    F = foreground_filter(gris, 130)\n",
    "    \n",
    "    if np.any(F):\n",
    "        return np.mean(gris[F])\n",
    "    else:\n",
    "        return np.mean(gris)\n",
    "\n",
    "paths = [\n",
    "    \"DATA/train/sain\",\n",
    "    \"DATA/train/malade\",\n",
    "    \"DATA/valid/sain\",\n",
    "    \"DATA/valid/malade\",\n",
    "]\n",
    "\n",
    "data = []\n",
    "\n",
    "for dossier in paths:\n",
    "    if os.path.exists(dossier):\n",
    "        image_files = [os.path.join(dossier, f) for f in os.listdir(dossier) if f.lower().endswith('.png')]\n",
    "        \n",
    "        for image_path in image_files:\n",
    "            with Image.open(image_path) as img:\n",
    "                lum = lightness(img)\n",
    "            data.append({\n",
    "                \"Image Index\": os.path.basename(image_path),\n",
    "                \"Folder\": dossier,\n",
    "                \"lightness\": lum\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"lightness.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Analyse des biais liés à la luminosité des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifions d'abord si la colonne 'lightness' existe dans le DataFrame\n",
    "if 'lightness' in df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['lightness'], kde=True, bins=30, color='blue')\n",
    "    plt.title('Distribution de la Luminosité (Lightness)', fontsize=16)\n",
    "    plt.xlabel('Lightness', fontsize=14)\n",
    "    plt.ylabel('Fréquence', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"La colonne 'lightness' n'existe pas dans le DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifions d'abord si la colonne 'lightness' existe dans le DataFrame\n",
    "if 'lightness' in df.columns and 'Folder' in df.columns:\n",
    "    df['labels'] = df['Folder'].apply(lambda x: 1 if 'malade' in x else 0)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(data=df, x='lightness', hue='labels', fill=True, common_norm=False, \n",
    "                palette={0: 'green', 1: 'red'}, alpha=0.5)\n",
    "    plt.title('Distribution de la Luminosité (Lightness) par État du Patient', fontsize=16)\n",
    "    plt.xlabel('Lightness', fontsize=14)\n",
    "    plt.ylabel('Densité', fontsize=14)\n",
    "    plt.legend(title='État du Patient', labels=['Sain', 'Malade'])\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Les colonnes 'lightness' ou 'Folder' n'existent pas dans le DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les distributions de la variable « lightness » pour les malades et les sains se chevauchent largement, ce qui indique qu'il n'existe pas de séparation nette entre les deux groupes. Cela suggère que la variation de la luminosité observée n'est pas le reflet d'un biais discriminatoire dans le modèle, mais plutôt d'une fluctuation technique inhérente aux réglages d'exposition ou à la standardisation des acquisitions radiographiques. En conséquence, on peut considérer que cette variable, en l'état, n'induit pas de biais dans la prise de décision, bien qu'il soit toujours recommandé de procéder à des analyses statistiques complémentaires pour confirmer que cette caractéristique n'influence pas indûment les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation du dataset AIF360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage sous format AIF360\n",
    "\n",
    "protected_attributes = ['Patient Gender', 'age_30_65']\n",
    "protected_attribute = protected_attributes[1]\n",
    "\n",
    "def get_aif360_data(df):\n",
    "    dff = df.copy()\n",
    "    dff[\"Patient Gender\"] = dff[\"Patient Gender\"].map({\"M\": 0, \"F\": 1})\n",
    "\n",
    "    ret_df = BinaryLabelDataset(\n",
    "        favorable_label=0,  # \"sain\" est la classe favorable\n",
    "        unfavorable_label=1,  # \"malade\" est la classe défavorable\n",
    "        df=dff,\n",
    "        label_names=['labels'], \n",
    "        protected_attribute_names=protected_attributes\n",
    "    )\n",
    "\n",
    "    ret_df_train = BinaryLabelDataset(\n",
    "        favorable_label=0,  # \"sain\" est la classe favorable\n",
    "        unfavorable_label=1,  # \"malade\" est la classe défavorable\n",
    "        df=dff[dff[\"in_train\"]==1],\n",
    "        label_names=['labels'], \n",
    "        protected_attribute_names=protected_attributes\n",
    "    )\n",
    "\n",
    "    ret_df_valid = BinaryLabelDataset(\n",
    "        favorable_label=0,  # \"sain\" est la classe favorable\n",
    "        unfavorable_label=1,  # \"malade\" est la classe défavorable\n",
    "        df=dff[dff[\"in_train\"]==0],\n",
    "        label_names=['labels'], \n",
    "        protected_attribute_names=protected_attributes\n",
    "    )\n",
    "\n",
    "    return ret_df, ret_df_train, ret_df_valid\n",
    "\n",
    "aif_df, aif_df_train, aif_df_valid = get_aif360_data(df_encoded_bool)\n",
    "\n",
    "aif_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(labels, preds=None, gender_or_age=0):\n",
    "    if gender_or_age == 0:\n",
    "        print(\"PROTECTED: Patient Gender\")\n",
    "        metrics_before_training = get_group_metrics(\n",
    "            y_true=labels,\n",
    "            y_pred=preds,\n",
    "            prot_attr=aif_df_valid.protected_attributes[:, 0],\n",
    "            priv_group=1,\n",
    "            pos_label=0\n",
    "        )\n",
    "\n",
    "        # Affichage des résultats\n",
    "        for metric, value in metrics_before_training.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"PROTECTED: age_30_65\")\n",
    "        metrics_before_training = get_group_metrics(\n",
    "            y_true=labels,\n",
    "            y_pred=preds,\n",
    "            prot_attr=aif_df_valid.protected_attributes[:, 1],\n",
    "            priv_group=1,\n",
    "            pos_label=0\n",
    "        )\n",
    "\n",
    "        # Affichage des résultats\n",
    "        for metric, value in metrics_before_training.items():\n",
    "            print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_Ys(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    train_test_split(df)\n",
    "    df[\"preds\"] = df[\"preds\"].map({\"sain\": 0, \"malade\": 1}) \n",
    "    df[\"labels\"] = df[\"labels\"].map({\"sain\": 0, \"malade\": 1})\n",
    "    test_df = df[df[\"in_train\"] == 0]\n",
    "    preds = test_df[\"preds\"]\n",
    "    labels= test_df[\"labels\"]\n",
    "    return labels, preds\n",
    "\n",
    "labels, preds = get_test_Ys(PREDS_DIR+\"/preds.csv\")\n",
    "\n",
    "print_metrics(labels, gender_or_age=0)\n",
    "print(\"====================\")\n",
    "print_metrics(labels, gender_or_age=1)\n",
    "\n",
    "# Explication des sorties:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(labels, preds, gender_or_age=0)\n",
    "print(\"====================\")\n",
    "print_metrics(labels, preds, gender_or_age=1)\n",
    "\n",
    "# Explication des sorties:\n",
    "\n",
    "\n",
    "# -> On se concentre plus sur l'age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_encoded_bool[\"labels\"]\n",
    "y_pred = df_encoded_bool[\"preds\"]\n",
    "plot_confusion_matrix(y_true, y_pred, normalize=True)\n",
    "\n",
    "# Explication des sorties:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_by_group(y_true, y_pred, df_encoded_bool, group_columns=[\"age_30_65\"], labels=[\"sain\", \"malade\"], normalize=True)\n",
    "\n",
    "# Explication des sorties:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.3. Identification des biais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résumé des biais identifiés précédemment:\n",
    "**a. Une sous-représentation des femmes dans le dataset:**\n",
    "*42% de femmes versus 58% d'hommes, n'est pas une grande différence, de plus, le disparate impact ratio (DIR)\n",
    "est très proche de 1 dans tous les cas, ce qui indique qu'il n’y a pas de biais significatif en termes de différence de traitement entre les hommes et les femmes.*\n",
    "\n",
    "**b. Une sous-représentation de quelques maladies dans le dataset (deséquilibre label):**\n",
    "*On remarque que quelques maladies sont sous-représentées (exemple: Hernia, Fibrosis, ...), d'autres sont sur-représentées (exemple: Infiltration, Iffusion, ...), cela peut entraîner un biais, car le modèle peut par exemple toujours prédire qu'un patient n'a pas Hernia (par exemple), sans être vraiment pénalisé, et donc ça peut donner de faux scores élevés.*\n",
    "\n",
    "**c. Une sous-représentation de quelques classes d'age:**\n",
    "*De même, dû à la sous-représentation des classes d'âge jeunes et vieilles, un modèle entrainé sur ce dataset peut donner des prédictions meilleures pour la classe du mileu [30-65 ans].*\n",
    "\n",
    "**d. Des anomalies liées à la distribution de patient_id:**\n",
    "*L'anomalie qu'on a dans la distribution des Patient IDs au voisinage de l'ID 25000 (probablement dûe à des données manquantes), peut donner un modèle qui répond toujours par 'is_ill = 0' (Patient n'est pas malade), si l'ID du patient est dans le voisinage de 25000, qui pose un problème.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Application des méthodes de preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique le Reweighing sur le dataset\n",
    "RW = Reweighing(unprivileged_groups=[{protected_attribute: 0}], privileged_groups=[{protected_attribute: 1}])\n",
    "RW.fit(aif_df_train)\n",
    "transformed_dataset_RW = RW.transform(aif_df)\n",
    "\n",
    "df_encoded_bool[\"WEIGHTS\"] = transformed_dataset_RW.instance_weights\n",
    "\n",
    "# On reconcatène les colonnes relatives aux images\n",
    "df_concatenated = pd.concat([df_image_related, df_encoded_bool], axis=1)\n",
    "\n",
    "# On écrit les poids dans les metadatas\n",
    "path_to_csv_RW = DATA_DIR+\"/metadata_RW.csv\"\n",
    "df_concatenated.to_csv(path_to_csv_RW, index=False)\n",
    "\n",
    "\n",
    "# On entraine le modele sur ces données (voir training.ipynb) -> resultat: preds_RW.csv\n",
    "\n",
    "\n",
    "# On charge les prédictions\n",
    "path_to_preds_RW = PREDS_DIR+\"/preds_RW.csv\"\n",
    "labels, preds = get_test_Ys(path_to_preds_RW)\n",
    "\n",
    "print_metrics(labels, preds, gender_or_age=0)\n",
    "print(\"====================\")\n",
    "print_metrics(labels, preds, gender_or_age=1)\n",
    "\n",
    "# Explication des sorties:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. DisparateImpactRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique le DIR sur le dataset\n",
    "DIR = DisparateImpactRemover(sensitive_attribute=protected_attribute,repair_level=1.0)\n",
    "transformed_dataset_DIR = DIR.fit_transform(aif_df)\n",
    "\n",
    "# On reconcatène les colonnes relatives aux images\n",
    "df_concatenated = pd.concat([df_image_related.reset_index(drop=True), \n",
    "    transformed_dataset_DIR.convert_to_dataframe()[0].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# On écrit les poids dans les metadatas\n",
    "path_to_csv_DIR = DATA_DIR+\"/metadata_DIR.csv\"\n",
    "df_concatenated.to_csv(path_to_csv_DIR, index=False)\n",
    "\n",
    "\n",
    "# On entraine le modele sur ces données (voir training.ipynb) -> resultat: preds_DIR.csv\n",
    "\n",
    "\n",
    "# On charge les prédictions\n",
    "path_to_preds_DIR = PREDS_DIR+\"/preds_DIR.csv\"\n",
    "labels, preds = get_test_Ys(path_to_preds_DIR)\n",
    "\n",
    "print_metrics(labels, preds, gender_or_age=0)\n",
    "print(\"====================\")\n",
    "print_metrics(labels, preds, gender_or_age=1)\n",
    "\n",
    "# Explication des sorties:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. LFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique le LFR sur le dataset\n",
    "LFR = LFR(\n",
    "    unprivileged_groups=[{protected_attribute: 0}],\n",
    "    privileged_groups=[{protected_attribute: 1}],\n",
    "    k=5,\n",
    "    Ax=0.01,\n",
    "    Ay=1.0,\n",
    "    Az=50.0,\n",
    "    print_interval=250,\n",
    "    verbose=1,\n",
    "    seed=None,\n",
    ")\n",
    "LFR.fit(aif_df_train, maxiter=5000, maxfun=5000)\n",
    "transformed_dataset = LFR.transform(aif_df)\n",
    "\n",
    "# On reconcatène les colonnes relatives aux images\n",
    "df_concatenated = pd.concat([df_image_related.reset_index(drop=True), \n",
    "    transformed_dataset_DIR.convert_to_dataframe()[0].reset_index(drop=True)], axis=1)\n",
    "\n",
    "# On écrit les poids dans les metadatas\n",
    "path_to_csv_LFR = DATA_DIR+\"/metadata_LFR.csv\"\n",
    "df_concatenated.to_csv(path_to_csv_LFR, index=False)\n",
    "\n",
    "\n",
    "# On entraine le modele sur ces données (voir training.ipynb) -> resultat: preds_DIR.csv\n",
    "\n",
    "\n",
    "# On charge les prédictions\n",
    "path_to_preds_LFR = PREDS_DIR+\"/preds_LFR.csv\"\n",
    "labels, preds = get_test_Ys(path_to_preds_LFR)\n",
    "\n",
    "print_metrics(labels, preds, gender_or_age=0)\n",
    "print(\"====================\")\n",
    "print_metrics(labels, preds, gender_or_age=1)\n",
    "\n",
    "# Explication des sorties:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Application des méthodes de postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les chemins vers les dossiers d'entraînement\n",
    "path_train_sain = DATA_DIR + \"/train/sain\"\n",
    "path_train_malade = DATA_DIR + \"/train/malade\"\n",
    "\n",
    "# Lister les noms de fichiers d’images présents dans les deux sous-dossiers d’entraînement\n",
    "images_entraînement  = set(os.listdir(path_train_sain) + os.listdir(path_train_malade))\n",
    "\n",
    "\n",
    "def convert_all_to_numericals(df):\n",
    "    # Nettoyer les noms de colonnes\n",
    "    df.columns = df.columns.str.strip()\n",
    "    \n",
    "    # Ajouter une colonne \"in_train\" si elle n'existe pas encore\n",
    "    if \"in_train\" not in df.columns:\n",
    "        df[\"in_train\"] = df[\"Image Index\"].apply(lambda x: 1 if x in images_entraînement else 0)\n",
    "    \n",
    "    # Convertir les prédictions en 0/1 si ce n'est pas déjà numérique\n",
    "    if not pd.api.types.is_numeric_dtype(df[\"preds\"]):\n",
    "        df[\"preds\"] = df[\"preds\"].map({\"sain\": 0, \"malade\": 1})\n",
    "    \n",
    "    # Convertir les étiquettes en 0/1 si nécessaire\n",
    "    if not pd.api.types.is_numeric_dtype(df[\"labels\"]):\n",
    "        df[\"labels\"] = df[\"labels\"].map({\"sain\": 0, \"malade\": 1})\n",
    "    \n",
    "    # Convertir le genre en 0/1 (0 = M, 1 = F)\n",
    "    if not pd.api.types.is_numeric_dtype(df[\"Patient Gender\"]):\n",
    "        df[\"Patient Gender\"] = df[\"Patient Gender\"].map({\"M\": 0, \"F\": 1})\n",
    "    \n",
    "    # Convertir la position de vue si elle est présente et non numérique\n",
    "    if \"View Position\" in df.columns and not pd.api.types.is_numeric_dtype(df[\"View Position\"]):\n",
    "        df[\"View Position\"] = df[\"View Position\"].map({\"AP\": 0, \"PA\": 1})\n",
    "    \n",
    "    # Ajouter une colonne \"+40ans\" indiquant si l'âge du patient est supérieur à 40\n",
    "    if \"+40ans\" not in df.columns:\n",
    "        df[\"+40ans\"] = (df[\"Patient Age\"] > 40).astype(int)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On numérise toutes les colonnes\n",
    "preddf = df_encoded_bool.copy()\n",
    "preddf = convert_all_to_numericals(preddf)\n",
    "\n",
    "\n",
    "protected_attributes = ['Patient Gender', '+40ans']\n",
    "\n",
    "protected_attribute = protected_attributes[1]\n",
    "priviliged_group = 0\n",
    "unpriviliged_group = 1\n",
    "\n",
    "if protected_attribute in list(preddf.columns):\n",
    "    dataset = BinaryLabelDataset(\n",
    "        favorable_label=1,  # \"Sain\" est la classe favorable\n",
    "        unfavorable_label=0,  # \"Malade\" est la classe défavorable\n",
    "        df=preddf,\n",
    "        label_names=[\"labels\"],\n",
    "        protected_attribute_names=[protected_attribute]\n",
    "    )\n",
    "\n",
    "    t = preddf[preddf[\"in_train\"]==1].copy().reset_index()\n",
    "    train_dataset = BinaryLabelDataset(\n",
    "        favorable_label=1,  # \"Sain\" est la classe favorable\n",
    "        unfavorable_label=0,  # \"Malade\" est la classe défavorable\n",
    "        df=t,\n",
    "        label_names=[\"labels\"],\n",
    "        protected_attribute_names=[protected_attribute]\n",
    "    )\n",
    "\n",
    "\n",
    "    d = preddf[preddf[\"in_train\"]==0].copy().reset_index()\n",
    "    test_dataset = BinaryLabelDataset(\n",
    "        favorable_label=1,  # \"Sain\" est la classe favorable\n",
    "        unfavorable_label=0,  # \"Malade\" est la classe défavorable\n",
    "        df=d,\n",
    "        label_names=[\"labels\"],\n",
    "        protected_attribute_names=[protected_attribute]\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(f\"Colonne protégée '{protected_attribute}' non trouvée dans le dataset.\")\n",
    "\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_values = {\n",
    "    \"statistical_parity_difference\": 0.0,\n",
    "    \"disparate_impact_ratio\": 1.0,\n",
    "    \"equal_opportunity_difference\": 0.0,\n",
    "    \"average_odds_difference\": 0.0,\n",
    "    \"conditional_demographic_disparity\": 0.0,\n",
    "    \"smoothed_edf\": 1.0,\n",
    "    \"df_bias_amplification\": 0.0\n",
    "}\n",
    "\"\"\"\n",
    "def check_distance_to_ideal(metrics):\n",
    "    dist = 0.0\n",
    "    for metric in ideal_values.keys():\n",
    "        dist += (ideal_values[metric]-metrics[metric])**2\n",
    "    return np.sqrt(dist)\n",
    "\n",
    "def calculate_metrics(csv_name):\n",
    "    preddf = pd.read_csv(\"./expe_log/\"+csv_name)\n",
    "    preddf = convert_all_to_numericals(preddf)\n",
    "    test_df = preddf[preddf[\"in_train\"] == 0]\n",
    "\n",
    "    \n",
    "    preds = test_df[\"preds\"]\n",
    "    labels= test_df[\"labels\"]\n",
    "    weights = test_df[\"WEIGHTS\"]\n",
    "\n",
    "\n",
    "    metrics_after = get_group_metrics(\n",
    "        y_true=labels,\n",
    "        y_pred=preds,\n",
    "        prot_attr=test_df[protected_attribute],\n",
    "        priv_group=1,\n",
    "        pos_label=1,\n",
    "        sample_weight= weights if csv_name==\"preds.csv\" else None\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Affichage des résultats de fairness\n",
    "    for metric, value in metrics_after.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    print(\"Distance to ideal:\",check_distance_to_ideal(metrics_after))\n",
    "\n",
    "    return metrics_after,test_df\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = preddf[preddf[\"in_train\"] == 0]\n",
    "preds = test_df[\"preds\"]\n",
    "labels= test_df[\"labels\"]\n",
    "\n",
    "metrics_before = get_group_metrics(\n",
    "    y_true=labels,\n",
    "    y_pred=preds,\n",
    "    prot_attr=test_dataset.protected_attributes[:, 0],\n",
    "    priv_group=1,\n",
    "    pos_label=0\n",
    ")\n",
    "\n",
    "# Affichage des résultats\n",
    "for metric, value in metrics_before.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rw = pd.read_csv(\"./expe_log/preds_RW.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(metric_orig,  metric_transf):\n",
    "    print(\"\\nMétriques avant ROC:\")\n",
    "    print(\"Equal Opportunity Diff:\", metric_orig.equal_opportunity_difference())\n",
    "    print(\"Disparate Impact:\", metric_orig.disparate_impact())\n",
    "    print(\"Average Odds Difference:\", metric_orig.average_odds_difference())\n",
    "    print(\"Theil Index:\", metric_orig.theil_index())\n",
    "    print(\"Statistical Parity Difference:\", metric_orig.statistical_parity_difference())\n",
    "    print(\"Error Rate:\", metric_orig.error_rate())\n",
    "    print(\"False Positive Rate:\", metric_orig.false_positive_rate())\n",
    "    print(\"False Negative Rate:\", metric_orig.false_negative_rate())\n",
    "    print(\"True Positive Rate:\", metric_orig.true_positive_rate())\n",
    "    print(\"True Negative Rate:\", metric_orig.true_negative_rate())\n",
    "\n",
    "    print(\"\\nMétriques après ROC:\")\n",
    "    print(\"Equal Opportunity Diff:\", metric_transf.equal_opportunity_difference())\n",
    "    print(\"Disparate Impact:\", metric_transf.disparate_impact())\n",
    "    print(\"Average Odds Difference:\", metric_transf.average_odds_difference())\n",
    "    print(\"Theil Index:\", metric_transf.theil_index())\n",
    "    print(\"Statistical Parity Difference:\", metric_transf.statistical_parity_difference())\n",
    "    print(\"Error Rate:\", metric_transf.error_rate())\n",
    "    print(\"False Positive Rate:\", metric_transf.false_positive_rate())\n",
    "    print(\"False Negative Rate:\", metric_transf.false_negative_rate())\n",
    "    print(\"True Positive Rate:\", metric_transf.true_positive_rate())\n",
    "    print(\"True Negative Rate:\", metric_transf.true_negative_rate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Reject Option Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = convert_all_to_numericals(test_df)\n",
    "\n",
    "# Attribut protégé et groupes\n",
    "protected_attribute = '+40ans'\n",
    "privileged_groups = [{protected_attribute: 1}]\n",
    "unprivileged_groups = [{protected_attribute: 0}]\n",
    "\n",
    "# Créer les BinaryLabelDataset\n",
    "test_dataset = BinaryLabelDataset(\n",
    "    favorable_label=0,\n",
    "    unfavorable_label=1,\n",
    "    df=test_df.select_dtypes(include=['int64', 'float64']),\n",
    "    label_names=[\"labels\"],\n",
    "    protected_attribute_names=[protected_attribute]\n",
    ")\n",
    "\n",
    "# Injecter prédictions et logits\n",
    "test_with_preds = test_dataset.copy()\n",
    "test_with_preds.labels = test_df[\"preds\"].values.reshape(-1, 1)\n",
    "test_with_preds.scores = test_df[\"logits_1\"].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On va travailler sur le dataset des prédictions sans préprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guided_ROC_search(\n",
    "    test_dataset,\n",
    "    test_with_preds,\n",
    "    unprivileged_groups,\n",
    "    privileged_groups,\n",
    "    start_low=0.05,\n",
    "    high=0.99,\n",
    "    margin=10,\n",
    "    step=0.01,\n",
    "    max_iter=20,\n",
    "    fairness_bounds=(-0.05, 0.05)\n",
    "):\n",
    "    current_low = start_low\n",
    "    best_fn = float('inf')\n",
    "    best_config = None\n",
    "    history = []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        ROC = RejectOptionClassification(\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups,\n",
    "            low_class_thresh=current_low,\n",
    "            high_class_thresh=high,\n",
    "            num_class_thresh=100,\n",
    "            num_ROC_margin=margin,\n",
    "            metric_name=\"Equal opportunity difference\",\n",
    "            metric_lb=fairness_bounds[0],\n",
    "            metric_ub=fairness_bounds[1]\n",
    "        )\n",
    "\n",
    "        ROC = ROC.fit(test_dataset, test_with_preds)\n",
    "        transformed = ROC.predict(test_with_preds)\n",
    "\n",
    "        y_true = test_dataset.labels.ravel()\n",
    "        y_pred = transformed.labels.ravel()\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "        metric = ClassificationMetric(\n",
    "            test_dataset,\n",
    "            transformed,\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups\n",
    "        )\n",
    "        fairness_diff = metric.equal_opportunity_difference()\n",
    "\n",
    "        history.append({\n",
    "            'iteration': i,\n",
    "            'low_thresh': round(current_low, 3),\n",
    "            'FN': fn,\n",
    "            'FP': fp,\n",
    "            'TP': tp,\n",
    "            'TN': tn,\n",
    "            'fairness_diff': fairness_diff\n",
    "        })\n",
    "\n",
    "        # Garder le meilleur si fairness respectée\n",
    "        if fairness_bounds[0] <= fairness_diff <= fairness_bounds[1]:\n",
    "            if fn < best_fn:\n",
    "                best_fn = fn\n",
    "                best_config = {\n",
    "                    'low_thresh': current_low,\n",
    "                    'FN': fn,\n",
    "                    'FP': fp,\n",
    "                    'TP': tp,\n",
    "                    'TN': tn,\n",
    "                    'fairness_diff': fairness_diff\n",
    "                }\n",
    "\n",
    "        # Avancer quoi qu'il arrive\n",
    "        current_low -= step\n",
    "        if current_low < 0.0:\n",
    "            break\n",
    "\n",
    "    return best_config, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config, search_history = guided_ROC_search(\n",
    "    test_dataset,\n",
    "    test_with_preds,\n",
    "    unprivileged_groups,\n",
    "    privileged_groups,\n",
    "    start_low=0.1,\n",
    "    high=0.9,\n",
    "    margin=50\n",
    ")\n",
    "\n",
    "print(\"Best config found:\")\n",
    "print(best_config)\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(search_history))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = RejectOptionClassification(\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups,\n",
    "    low_class_thresh=best_config['low_thresh'],\n",
    "    high_class_thresh=0.99,\n",
    "    num_class_thresh=100,\n",
    "    num_ROC_margin=50,\n",
    "    metric_name=\"Equal opportunity difference\",\n",
    "    metric_lb=-0.05,\n",
    "    metric_ub=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = ROC.fit(test_dataset, test_with_preds)\n",
    "transformed_dataset = ROC.predict(test_with_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Matrice de confusion avant et après transformation\n",
    "conf_matrix_orig = confusion_matrix(test_dataset.labels, test_with_preds.labels)\n",
    "conf_matrix_transf = confusion_matrix(test_dataset.labels, transformed_dataset.labels)\n",
    "\n",
    "# Normalisation en pourcentages\n",
    "conf_matrix_orig_percentage = conf_matrix_orig / conf_matrix_orig.sum(axis=1)[:, np.newaxis] * 100\n",
    "conf_matrix_transf_percentage = conf_matrix_transf / conf_matrix_transf.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Création des heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Heatmap avant transformation (en pourcentages)\n",
    "sns.heatmap(conf_matrix_orig_percentage, annot=True, fmt=\".2f\", cmap=\"Blues\", \n",
    "            xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "            yticklabels=[\"True Negative\", \"True Positive\"], ax=axes[0], cbar_kws={'label': 'Percentage'})\n",
    "axes[0].set_title('Matrice de Confusion Avant Transformation (en %)')\n",
    "axes[0].set_xlabel('Prédictions')\n",
    "axes[0].set_ylabel('Vraies étiquettes')\n",
    "\n",
    "# Heatmap après transformation (en pourcentages)\n",
    "sns.heatmap(conf_matrix_transf_percentage, annot=True, fmt=\".2f\", cmap=\"Blues\", \n",
    "            xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "            yticklabels=[\"True Negative\", \"True Positive\"], ax=axes[1], cbar_kws={'label': 'Percentage'})\n",
    "axes[1].set_title('Matrice de Confusion Après Transformation (en %)')\n",
    "axes[1].set_xlabel('Prédictions')\n",
    "axes[1].set_ylabel('Vraies étiquettes')\n",
    "\n",
    "# Affichage\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "metric_orig = ClassificationMetric(test_dataset, test_with_preds,\n",
    "                                   unprivileged_groups=unprivileged_groups,\n",
    "                                   privileged_groups=privileged_groups)\n",
    "\n",
    "metric_transf = ClassificationMetric(test_dataset, transformed_dataset,\n",
    "                                     unprivileged_groups=unprivileged_groups,\n",
    "                                     privileged_groups=privileged_groups)\n",
    "\n",
    "\n",
    "printMetrics(metric_orig,  metric_transf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Aplatir les tableaux au cas où\n",
    "labels_before = test_with_preds.labels.ravel()\n",
    "labels_after = transformed_dataset.labels.ravel()\n",
    "\n",
    "# Tracer avec des histogrammes côte à côte\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.kdeplot(labels_before, color='blue', label='Avant ROC')\n",
    "sns.kdeplot(labels_after, color='orange', label='Après ROC')\n",
    "\n",
    "\n",
    "# Ajouter titres et légende propres\n",
    "plt.title(\"Distribution des prédictions (Avant vs Après ROC)\")\n",
    "plt.xlabel(\"Label prédit (0 = sain, 1 = malade)\")\n",
    "plt.ylabel(\"Nombre d'occurrences\")\n",
    "plt.legend()\n",
    "plt.xticks([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avec reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rw = convert_all_to_numericals(pred_rw)\n",
    "\n",
    "# Attribut protégé et groupes\n",
    "protected_attribute = '+40ans'\n",
    "privileged_groups = [{protected_attribute: 0}]\n",
    "unprivileged_groups = [{protected_attribute: 1}]\n",
    "\n",
    "# Créer les BinaryLabelDataset\n",
    "test_dataset_rw = BinaryLabelDataset(\n",
    "    favorable_label=0,\n",
    "    unfavorable_label=1,\n",
    "    df=pred_rw.select_dtypes(include=['int64', 'float64']),\n",
    "    label_names=[\"labels\"],\n",
    "    protected_attribute_names=[protected_attribute]\n",
    ")\n",
    "\n",
    "# Injecter prédictions et logits\n",
    "test_with_preds = test_dataset_rw.copy()\n",
    "test_with_preds.labels = pred_rw[\"preds\"].values.reshape(-1, 1)\n",
    "test_with_preds.scores = pred_rw[\"logits_1\"].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config, search_history = guided_ROC_search(\n",
    "    test_dataset_rw,\n",
    "    test_with_preds,\n",
    "    unprivileged_groups,\n",
    "    privileged_groups,\n",
    "    start_low=0.1,\n",
    "    high=0.9,\n",
    ")\n",
    "\n",
    "print(\"Best config found:\")\n",
    "print(best_config)\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(search_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC = ROC.fit(test_dataset_rw, test_with_preds)\n",
    "transformed_dataset = ROC.predict(test_with_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Matrice de confusion avant et après transformation\n",
    "conf_matrix_orig = confusion_matrix(test_dataset_rw.labels, test_with_preds.labels)\n",
    "conf_matrix_transf = confusion_matrix(test_dataset_rw.labels, transformed_dataset.labels)\n",
    "\n",
    "# Normalisation en pourcentages\n",
    "conf_matrix_orig_percentage = conf_matrix_orig / conf_matrix_orig.sum(axis=1)[:, np.newaxis] * 100\n",
    "conf_matrix_transf_percentage = conf_matrix_transf / conf_matrix_transf.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Création des heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Heatmap avant transformation (en pourcentages)\n",
    "sns.heatmap(conf_matrix_orig_percentage, annot=True, fmt=\".2f\", cmap=\"Blues\", \n",
    "            xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "            yticklabels=[\"True Negative\", \"True Positive\"], ax=axes[0], cbar_kws={'label': 'Percentage'})\n",
    "axes[0].set_title('Matrice de Confusion Avant Transformation (en %)')\n",
    "axes[0].set_xlabel('Prédictions')\n",
    "axes[0].set_ylabel('Vraies étiquettes')\n",
    "\n",
    "# Heatmap après transformation (en pourcentages)\n",
    "sns.heatmap(conf_matrix_transf_percentage, annot=True, fmt=\".2f\", cmap=\"Blues\", \n",
    "            xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "            yticklabels=[\"True Negative\", \"True Positive\"], ax=axes[1], cbar_kws={'label': 'Percentage'})\n",
    "axes[1].set_title('Matrice de Confusion Après Transformation (en %)')\n",
    "axes[1].set_xlabel('Prédictions')\n",
    "axes[1].set_ylabel('Vraies étiquettes')\n",
    "\n",
    "# Affichage\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des métriques\n",
    "metric_orig = ClassificationMetric(test_dataset_rw, test_with_preds,\n",
    "                                   unprivileged_groups=unprivileged_groups,\n",
    "                                   privileged_groups=privileged_groups)\n",
    "\n",
    "metric_transf = ClassificationMetric(test_dataset_rw, transformed_dataset,\n",
    "                                     unprivileged_groups=unprivileged_groups,\n",
    "                                     privileged_groups=privileged_groups)\n",
    "\n",
    "printMetrics(metric_orig,  metric_transf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Aplatir les tableaux au cas où\n",
    "labels_before = test_with_preds.labels.ravel()\n",
    "labels_after = transformed_dataset.labels.ravel()\n",
    "\n",
    "# Tracer avec des histogrammes côte à côte\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "sns.kdeplot(labels_before, color='blue', label='Avant ROC')\n",
    "sns.kdeplot(labels_after, color='orange', label='Après ROC')\n",
    "\n",
    "\n",
    "# Ajouter titres et légende propres\n",
    "plt.title(\"Distribution des prédictions (Avant vs Après ROC)\")\n",
    "plt.xlabel(\"Label prédit (0 = sain, 1 = malade)\")\n",
    "plt.ylabel(\"Nombre d'occurrences\")\n",
    "plt.legend()\n",
    "plt.xticks([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Calibrated Equalized Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_CEO(test_dataset, priv_group=1, pos_label=0):\n",
    "    cost_constraint = \"fnr\"\n",
    "\n",
    "    # Postprocessing CEO\n",
    "    ceo = CalibratedEqOddsPostprocessing(\n",
    "        privileged_groups=[{protected_attribute: priv_group}],\n",
    "        unprivileged_groups=[{protected_attribute: 1 - priv_group}],\n",
    "        cost_constraint=cost_constraint,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # On considère test_dataset comme les prédictions (avec .scores et .labels modifiés)\n",
    "    pred_dataset = test_dataset.copy()\n",
    "\n",
    "    # Fit + Predict\n",
    "    ceo = ceo.fit(test_dataset, pred_dataset)\n",
    "    postproc_preds = ceo.predict(pred_dataset)\n",
    "\n",
    "    # Calcul des métriques de fairness\n",
    "    metrics = get_group_metrics(\n",
    "        y_true=test_dataset.labels[:, 0],\n",
    "        y_pred=postproc_preds.labels[:, 0],\n",
    "        prot_attr=test_dataset.protected_attributes[:, 0],\n",
    "        priv_group=priv_group,\n",
    "        pos_label=pos_label\n",
    "    )\n",
    "\n",
    "    # Convertir les métriques en float\n",
    "    metrics_float = {metric: float(value) for metric, value in metrics.items()}\n",
    "\n",
    "    return metrics_float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_CEO(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_CEO(test_dataset_rw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Analyse et compréhension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -> Get together and link preproc and postproc results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Preferably*, get together and discuss final comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
